{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 241 rows and 4 columns.\n",
      "The four columns are id, role, company, description.\n",
      "Number of unique roles: 4\n",
      "Number of unique companies: 50\n",
      "There are 65 data analyst roles.\n",
      "There are 61 data engineer roles.\n",
      "There are 60 machine learning engineer roles.\n",
      "There are 55 data scientist roles.\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the job data\n",
    "job_data = []\n",
    "\n",
    "# Iterate through job data files and extract job information\n",
    "for role_name in [\"data scientist\", \"data analyst\", \"data engineer\", \"machine learning engineer\"]:\n",
    "    for company in [\"apple\", \"google\", \"microsoft\", \"facebook\", \"tesla\", \"amazon\", \"UT Health Science Center at San Antonio\"]:\n",
    "        folder_path = f\"raw_data/{role_name.replace(' ', '-')}/{company.replace(' ', '-')}\"\n",
    "        for filename in os.listdir(folder_path):\n",
    "            with open(os.path.join(folder_path, filename), 'r') as f:\n",
    "                job = json.load(f)\n",
    "                job_data.append({\n",
    "                    'id': job[\"id\"],\n",
    "                    'role': role_name,\n",
    "                    'company': job[\"company\"],\n",
    "                    'description': job[\"description\"]\n",
    "                })\n",
    "\n",
    "# Create a pandas DataFrame from the job data\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns\" + \".\")\n",
    "print(\"The four columns are \" + \", \".join(df.columns) + \".\")\n",
    "\n",
    "print(\"Number of unique roles:\", df[\"role\"].nunique())\n",
    "print(\"Number of unique companies:\", df[\"company\"].nunique())\n",
    "role_counts = df[\"role\"].value_counts().to_dict()\n",
    "\n",
    "for role, count in role_counts.items():\n",
    "    print(f\"There are {count} {role} roles.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2021), ('experience', 799), ('business', 635), ('Experience', 619), ('Data', 591), ('team', 511), ('work', 468), ('Apple', 392), ('role', 373), ('including', 371)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model for spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Combine all job descriptions into one long string\n",
    "descriptions = \" \".join(df[\"description\"].values)\n",
    "\n",
    "# Tokenize the combined string using spaCy\n",
    "doc = nlp(descriptions)\n",
    "\n",
    "# Create a counter object to count the frequency of each token\n",
    "token_freq = Counter(token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space)\n",
    "\n",
    "# Print the top 10 most common tokens\n",
    "print(token_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('machine', 'learning'), 354), (('base', 'pay'), 165), (('+', 'years'), 163), (('computer', 'science'), 144), (('data', 'science'), 142), (('employee', 'stock'), 99), (('preferred', 'qualifications'), 94), (('data', 'center'), 88), (('equal', 'employment'), 84), (('equal', 'opportunity'), 83)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model for spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Combine all job descriptions into one long string\n",
    "descriptions = \" \".join(df[\"description\"].values)\n",
    "\n",
    "# Tokenize the combined string using spaCy\n",
    "doc = nlp(descriptions)\n",
    "\n",
    "# Create a counter object to count the frequency of each two-word phrase\n",
    "phrase_freq = Counter((doc[i].text.lower(), doc[i+1].text.lower()) for i in range(len(doc)-1) if not doc[i].is_stop and not doc[i+1].is_stop and not doc[i].is_punct and not doc[i+1].is_punct and not doc[i].is_space and not doc[i+1].is_space)\n",
    "\n",
    "# Print the top 10 most common two-word phrases\n",
    "print(phrase_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business: 167\n",
      "ML: 165\n",
      "python: 147\n",
      "SQL: 123\n",
      "AI: 54\n",
      "R: 52\n",
      "algorithms: 51\n",
      "big data: 49\n",
      "Tableau: 49\n",
      "java: 46\n",
      "deep learning: 44\n",
      "data analytics: 42\n",
      "data mining: 38\n",
      "NLP: 38\n",
      "data management: 34\n",
      "TensorFlow: 33\n",
      "automation: 24\n",
      "Power BI: 20\n",
      "PyTorch: 20\n",
      "forecasting: 19\n",
      "statistical analysis: 17\n",
      "predictive modeling: 10\n",
      "KPI: 8\n",
      "time series: 8\n",
      "statistical modeling: 7\n",
      "SAS: 7\n",
      "inference: 7\n",
      "theory: 6\n",
      "survival analysis: 1\n",
      "\n",
      "Total count: 1286\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define a dictionary of abbreviations\n",
    "abbreviations = {\n",
    "    \"machine learning\": \"ML\",\n",
    "    \"artificial intelligence\": \"AI\",\n",
    "    \"natural language processing\": \"NLP\"\n",
    "}\n",
    "\n",
    "# Define a list of phrases to search for\n",
    "phrases = [\"SQL\", \"R\", \"data mining\", \"python\", \"java\", \"machine learning\", \"ML\", \"natural language processing\", \"NLP\", \"deep learning\", \"data analytics\", \"predictive modeling\", \"forecasting\", \"business\", \"statistical analysis\", \"KPI\", \"artificial intelligence\", \"AI\", \"statistical modeling\", \"data management\", \"algorithms\", \"automation\", \"TensorFlow\", \"PyTorch\", \"big data\", \"SAS\", \"survival analysis\", \"time series\", \"theory\", \"inference\", \"Power BI\", \"Tableau\"]\n",
    "\n",
    "# Create a counter object to count the number of descriptions that contain each phrase\n",
    "phrase_count = Counter()\n",
    "\n",
    "# Iterate through each description and count the number of phrases it contains\n",
    "for desc in df[\"description\"].values:\n",
    "    desc_phrases = set()\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{phrase}\\b\", desc, re.IGNORECASE):\n",
    "            desc_phrases.add(phrase)\n",
    "    for phrase in desc_phrases:\n",
    "        if phrase in abbreviations:\n",
    "            phrase = abbreviations[phrase]\n",
    "        phrase_count[phrase] += 1\n",
    "\n",
    "# Print the phrases and the number of descriptions that contain them in descending order\n",
    "total_count = 0\n",
    "\n",
    "for phrase, count in phrase_count.most_common():\n",
    "    print(f\"{phrase}: {count}\")\n",
    "    total_count += count\n",
    "\n",
    "print(f\"\\nTotal count: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business: 69.29%\n",
      "ML: 68.46%\n",
      "python: 61.00%\n",
      "SQL: 51.04%\n",
      "AI: 22.41%\n",
      "R: 21.58%\n",
      "algorithms: 21.16%\n",
      "big data: 20.33%\n",
      "Tableau: 20.33%\n",
      "java: 19.09%\n",
      "deep learning: 18.26%\n",
      "data analytics: 17.43%\n",
      "data mining: 15.77%\n",
      "NLP: 15.77%\n",
      "data management: 14.11%\n",
      "TensorFlow: 13.69%\n",
      "Excel: 11.62%\n",
      "automation: 9.96%\n",
      "Power BI: 8.30%\n",
      "PyTorch: 8.30%\n",
      "forecasting: 7.88%\n",
      "statistical analysis: 7.05%\n",
      "predictive modeling: 4.15%\n",
      "KPI: 3.32%\n",
      "time series: 3.32%\n",
      "statistical modeling: 2.90%\n",
      "SAS: 2.90%\n",
      "inference: 2.90%\n",
      "survival analysis: 0.41%\n",
      "\n",
      "Total count: 241\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define a dictionary of abbreviations\n",
    "abbreviations = {\n",
    "    \"machine learning\": \"ML\",\n",
    "    \"artificial intelligence\": \"AI\",\n",
    "    \"natural language processing\": \"NLP\"\n",
    "}\n",
    "\n",
    "# Define a list of phrases to search for\n",
    "phrases = [\"Excel\", \"SQL\", \"R\", \"data mining\", \"python\", \"java\", \"machine learning\", \"ML\", \"natural language processing\", \"NLP\", \"deep learning\", \"data analytics\", \"predictive modeling\", \"forecasting\", \"business\", \"statistical analysis\", \"KPI\", \"artificial intelligence\", \"AI\", \"statistical modeling\", \"data management\", \"algorithms\", \"automation\", \"TensorFlow\", \"PyTorch\", \"big data\", \"SAS\", \"survival analysis\", \"time series\", \"inference\", \"Power BI\", \"Tableau\"]\n",
    "\n",
    "# Create a counter object to count the number of descriptions that contain each phrase\n",
    "phrase_count = Counter()\n",
    "\n",
    "# Iterate through each description and count the number of phrases it contains\n",
    "for desc in df[\"description\"].values:\n",
    "    desc_phrases = set()\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{phrase}\\b\", desc, re.IGNORECASE):\n",
    "            desc_phrases.add(phrase)\n",
    "    for phrase in desc_phrases:\n",
    "        if phrase in abbreviations:\n",
    "            phrase = abbreviations[phrase]\n",
    "        phrase_count[phrase] += 1\n",
    "\n",
    "# Print the phrases and the percentage of descriptions that contain them in descending order\n",
    "total_count = len(df)\n",
    "\n",
    "for phrase, count in phrase_count.most_common():\n",
    "    score = count / total_count\n",
    "    print(f\"{phrase}: {score:.2%}\")\n",
    "\n",
    "print(f\"\\nTotal count: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business: 167 (12.99%)\n",
      "ML: 165 (12.83%)\n",
      "python: 147 (11.43%)\n",
      "SQL: 123 (9.56%)\n",
      "AI: 54 (4.20%)\n",
      "R: 52 (4.04%)\n",
      "algorithms: 51 (3.97%)\n",
      "big data: 49 (3.81%)\n",
      "Tableau: 49 (3.81%)\n",
      "java: 46 (3.58%)\n",
      "deep learning: 44 (3.42%)\n",
      "data analytics: 42 (3.27%)\n",
      "data mining: 38 (2.95%)\n",
      "NLP: 38 (2.95%)\n",
      "data management: 34 (2.64%)\n",
      "TensorFlow: 33 (2.57%)\n",
      "automation: 24 (1.87%)\n",
      "Power BI: 20 (1.56%)\n",
      "PyTorch: 20 (1.56%)\n",
      "forecasting: 19 (1.48%)\n",
      "statistical analysis: 17 (1.32%)\n",
      "predictive modeling: 10 (0.78%)\n",
      "KPI: 8 (0.62%)\n",
      "time series: 8 (0.62%)\n",
      "statistical modeling: 7 (0.54%)\n",
      "SAS: 7 (0.54%)\n",
      "inference: 7 (0.54%)\n",
      "theory: 6 (0.47%)\n",
      "survival analysis: 1 (0.08%)\n",
      "\n",
      "Total count: 1286\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define a dictionary of abbreviations\n",
    "abbreviations = {\n",
    "    \"machine learning\": \"ML\",\n",
    "    \"artificial intelligence\": \"AI\",\n",
    "    \"natural language processing\": \"NLP\"\n",
    "}\n",
    "\n",
    "# Define a list of phrases to search for\n",
    "phrases = [\"SQL\", \"R\", \"data mining\", \"python\", \"java\", \"machine learning\", \"ML\", \"natural language processing\", \"NLP\", \"deep learning\", \"data analytics\", \"predictive modeling\", \"forecasting\", \"business\", \"statistical analysis\", \"KPI\", \"artificial intelligence\", \"AI\", \"statistical modeling\", \"data management\", \"algorithms\", \"automation\", \"TensorFlow\", \"PyTorch\", \"big data\", \"SAS\", \"survival analysis\", \"time series\", \"theory\", \"inference\", \"Power BI\", \"Tableau\"]\n",
    "\n",
    "# Create a counter object to count the number of descriptions that contain each phrase\n",
    "phrase_count = Counter()\n",
    "\n",
    "# Iterate through each description and count the number of phrases it contains\n",
    "for desc in df[\"description\"].values:\n",
    "    desc_phrases = set()\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{phrase}\\b\", desc, re.IGNORECASE):\n",
    "            desc_phrases.add(phrase)\n",
    "    for phrase in desc_phrases:\n",
    "        if phrase in abbreviations:\n",
    "            phrase = abbreviations[phrase]\n",
    "        phrase_count[phrase] += 1\n",
    "\n",
    "# Print the phrases, the number of descriptions that contain them, and their score in descending order\n",
    "total_count = sum(phrase_count.values())\n",
    "\n",
    "for phrase, count in phrase_count.most_common():\n",
    "    score = count / total_count\n",
    "    print(f\"{phrase}: {count} ({score:.2%})\")\n",
    "\n",
    "print(f\"\\nTotal count: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data analytics: 5\n",
      "business: 5\n",
      "predictive modeling: 4\n",
      "data management: 4\n",
      "KPI: 4\n",
      "SQL: 4\n",
      "forecasting: 3\n",
      "SAS: 2\n",
      "R: 2\n",
      "java: 2\n",
      "python: 2\n",
      "data mining: 2\n",
      "statistical analysis: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define a list of phrases to search for\n",
    "phrases = [\"SQL\", \"R\", \"data mining\", \"python\", \"java\", \"machine learning\", \"ML\", \"natural language processing\", \"NLP\", \"deep learning\", \"data analytics\", \"predictive modeling\", \"forecasting\", \"business\", \"statistical analysis\", \"KPI\", \"artificial intelligence\", r\"\\bAI\\b\", \"statistical modeling\", \"data management\", \"algorithms\", \"automation\", \"TensorFlow\", \"PyTorch\", \"big data\", \"SAS\", \"survival analysis\", \"time series\"]\n",
    "\n",
    "# Create a counter object to count the number of descriptions that contain each phrase\n",
    "phrase_count = Counter()\n",
    "\n",
    "# Iterate through each description associated with the desired company and count the number of phrases it contains\n",
    "for desc in df[df[\"company\"] == \"UT Health San Antonio\"][\"description\"].values:\n",
    "    desc_phrases = set()\n",
    "    for phrase in phrases:\n",
    "        if re.search(rf\"\\b{phrase}\\b\", desc, re.IGNORECASE):\n",
    "            desc_phrases.add(phrase)\n",
    "    for phrase in desc_phrases:\n",
    "        phrase_count[phrase] += 1\n",
    "\n",
    "# Print the phrases and the number of descriptions that contain them in descending order\n",
    "for phrase, count in phrase_count.most_common():\n",
    "    print(f\"{phrase}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 tokens for data scientist:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Print the top 10 most common tokens for the current role\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTop 10 tokens for \u001b[39m\u001b[39m{\u001b[39;00mrole\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfor\u001b[39;00m token, count \u001b[39min\u001b[39;00m token_freq\u001b[39m.\u001b[39;49mmost_common(\u001b[39m10\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mcount\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model for spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create an empty dictionary to store token counts for each job role\n",
    "token_counts_by_role = {}\n",
    "\n",
    "# Iterate over each job role\n",
    "for role in df[\"role\"].unique():\n",
    "    # Combine all job descriptions for the current role into one long string\n",
    "    descriptions = \" \".join(df[df[\"role\"] == role][\"description\"].values)\n",
    "\n",
    "    # Tokenize the combined string using spaCy\n",
    "    doc = nlp(descriptions)\n",
    "\n",
    "    # Create a counter object to count the frequency of each token\n",
    "    token_freq = Counter(token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space)\n",
    "\n",
    "    # Convert abbreviations back into full phrases for readability\n",
    "    token_freq = {key: abbreviations.get(key, key) + f\" ({count})\" for key, count in token_freq.items()}\n",
    "\n",
    "    # Store the token counts for the current role in the dictionary\n",
    "    token_counts_by_role[role] = token_freq\n",
    "\n",
    "    # Print the top 10 most common tokens for the current role\n",
    "    print(f\"Top 10 tokens for {role}:\")\n",
    "    for token, count in token_freq.most_common(10):\n",
    "        print(f\"{token}: {count}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff6447c242370727920b636702d2302bcf5d3d688ef858bd3c4922b4119e433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
